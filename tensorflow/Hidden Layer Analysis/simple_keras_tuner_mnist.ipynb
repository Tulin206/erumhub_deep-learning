{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Setup and Imports\n",
    "# This section imports all necessary libraries:\n",
    "# - TensorFlow and Keras for deep learning\n",
    "# - Keras Tuner for hyperparameter optimization\n",
    "# - NumPy for numerical operations\n",
    "# - Sklearn for data preprocessing utilities"
   ],
   "id": "566e4d4a4f27c86b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # 1) Install\n",
    "# !pip install -q -U keras-tuner"
   ],
   "id": "e13da27b55329ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load and Prepare MNIST Dataset\n",
    "# The MNIST dataset contains 70,000 grayscale images of handwritten digits (0-9)\n",
    "# Each image is 28x28 pixels, split into:\n",
    "# - 60,000 training images\n",
    "# - 10,000 test images"
   ],
   "id": "b02856c9670e4d63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load MNIST data\n",
    "print(\"Loading MNIST data...\")\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ],
   "id": "7497421dc701f7d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Preprocessing - Reshape and Normalize\n",
    "# Transform the data to make it suitable for neural network training:\n",
    "# 1. Reshape 28x28 images into 784 pixel vectors (flattening)\n",
    "# 2. Normalize pixel values from [0-255] to [0-1] range for better training"
   ],
   "id": "8619d3609d1319d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocess the data\n",
    "# Reshape and normalize the images\n",
    "X_train = X_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28).astype('float32') / 255.0"
   ],
   "id": "a19375ff47e959dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert Labels to One-Hot Encoding\n",
    "# Transform numerical labels (0-9) into one-hot encoded vectors\n",
    "# Example: 5 becomes [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "# This is necessary for multi-class classification"
   ],
   "id": "40748615ec1460b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert labels to categorical\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ],
   "id": "5c9c0ecba1769925"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display Data Shapes\n",
    "# Print the dimensions of training and test datasets to verify\n",
    "# the preprocessing steps were successful"
   ],
   "id": "37b710f33fed0b6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ],
   "id": "8ba6189b7109508c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define Model Building Function with Hyperparameter Tuning\n",
    "# This function creates a model architecture with tunable hyperparameters:\n",
    "# - Number of hidden layers (1-3)\n",
    "# - Neurons per layer (32-512)\n",
    "# - Dropout rates (0-0.5)\n",
    "# - Learning rate (1e-4 to 1e-2)"
   ],
   "id": "eefc664f50ee325c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    This function builds a model with tunable parameters:\n",
    "    1. Number of hidden layers (1-3)\n",
    "    2. Number of neurons in each layer (32-512)\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Input layer (flatten 28x28 images)\n",
    "    model.add(keras.layers.Input(shape=(784,)))  # 28*28 = 784\n",
    "\n",
    "    # Tune number of hidden layers (between 1 and 3)\n",
    "    n_layers = hp.Int('num_layers', min_value=1, max_value=3)\n",
    "\n",
    "    # Add hidden layers with tunable number of neurons\n",
    "    for i in range(n_layers):\n",
    "        # Number of neurons in this layer (32 to 512)\n",
    "        units = hp.Int(\n",
    "            f'units_layer_{i}',\n",
    "            min_value=32,\n",
    "            max_value=512,\n",
    "            step=32\n",
    "        )\n",
    "\n",
    "        # Add Dense layer with the tuned number of neurons\n",
    "        model.add(keras.layers.Dense(\n",
    "            units=units,\n",
    "            activation='relu'\n",
    "        ))\n",
    "\n",
    "        # Add Dropout for regularization\n",
    "        dropout_rate = hp.Float(\n",
    "            f'dropout_{i}',\n",
    "            min_value=0.0,\n",
    "            max_value=0.5,\n",
    "            step=0.1\n",
    "        )\n",
    "        model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer (10 neurons for digits 0-9)\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Tune learning rate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "id": "767acfb306d3e6d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize Keras Tuner\n",
    "# Set up RandomSearch tuner to explore different model configurations:\n",
    "# - Evaluates models based on validation accuracy\n",
    "# - Performs 5 trials with different hyperparameter combinations\n",
    "# - Stores results in 'keras_tuner/mnist_tuning' directory"
   ],
   "id": "6c1ded0f93a0057"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a tuner\n",
    "print(\"Creating tuner...\")\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',  # Changed to accuracy for classification\n",
    "    max_trials=5,\n",
    "    directory='keras_tuner',\n",
    "    project_name='mnist_tuning'\n",
    ")"
   ],
   "id": "f00a4b1aa89f60b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display Search Space Information\n",
    "# Show a summary of all hyperparameters being tuned\n",
    "# and their respective ranges"
   ],
   "id": "550dc6961b2230d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show search space summary\n",
    "print(\"\\nSearch space summary:\")\n",
    "tuner.search_space_summary()"
   ],
   "id": "edde6987e709193c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set Up Training Callbacks\n",
    "# Configure training optimizations:\n",
    "# 1. Early Stopping - Prevents overfitting by monitoring validation accuracy\n",
    "# 2. ReduceLROnPlateau - Reduces learning rate when progress stalls"
   ],
   "id": "3740ecb67e89576"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add early stopping\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]"
   ],
   "id": "e29617a9e91f4569"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Begin Hyperparameter Search\n",
    "# Start the hyperparameter optimization process:\n",
    "# - Trains models for 10 epochs each\n",
    "# - Uses batch size of 128 for efficient training\n",
    "# - 20% of training data used for validation"
   ],
   "id": "78641eb633d342c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Start the search\n",
    "print(\"\\nStarting the search...\")\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=10,  # Reduced epochs for MNIST\n",
    "    batch_size=128,  # Increased batch size for faster training\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "ccb569bd3b765f27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get Best Hyperparameters\n",
    "# Retrieve the hyperparameter configuration that achieved\n",
    "# the best validation accuracy during the search"
   ],
   "id": "45325ce659874396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ],
   "id": "1973f92999d14274"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display Best Hyperparameter Configuration\n",
    "# Print detailed information about the best model structure:\n",
    "# - Number of layers\n",
    "# - Neurons per layer\n",
    "# - Dropout rates\n",
    "# - Optimal learning rate"
   ],
   "id": "d956fc3ce5d467a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print the results\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "print(f\"Number of hidden layers: {best_hps.get('num_layers')}\")\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(f\"Layer {i+1}:\")\n",
    "    print(f\"  Units: {best_hps.get(f'units_layer_{i}')}\")\n",
    "    print(f\"  Dropout rate: {best_hps.get(f'dropout_{i}')}\")\n",
    "print(f\"Learning rate: {best_hps.get('learning_rate')}\")"
   ],
   "id": "93710fdb23898913"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build Final Model with Best Hyperparameters\n",
    "# Create a new model using the optimal hyperparameter configuration"
   ],
   "id": "4752fcd158ee017d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build the model with the best hyperparameters\n",
    "best_model = build_model(best_hps)"
   ],
   "id": "d345901dbca772df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train Final Model\n",
    "# Train the best model configuration for a longer duration:\n",
    "# - 20 epochs for better convergence\n",
    "# - Using the same callbacks for optimization"
   ],
   "id": "8020e77bc20d0854"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "print(\"\\nTraining the final model with best hyperparameters...\")\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,  # Train for more epochs on final model\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "66e920e6fb7dca9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate Model Performance\n",
    "# Test the final model on unseen test data to\n",
    "# measure its true performance and generalization"
   ],
   "id": "7e5b226851e8a735"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model\n",
    "print(\"\\nEvaluating the model...\")\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")"
   ],
   "id": "e4addb8f00ec6dac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save Trained Model\n",
    "# Save the trained model to disk for future use\n",
    "# The model is saved in Keras format (.keras)"
   ],
   "id": "5721f8e701bc0a98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the model\n",
    "model_path = 'best_mnist_model.keras'\n",
    "best_model.save(model_path)\n",
    "print(f\"\\nModel saved as '{model_path}'\")"
   ],
   "id": "7be8910150cf34d0"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
