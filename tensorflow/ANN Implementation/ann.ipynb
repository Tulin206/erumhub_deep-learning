{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d099110",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": "Artificial Neural Network for Customer Churn Prediction"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook demonstrates how to build a neural network to predict customer churn in a bank. We'll go through data preprocessing, model building, training, and evaluation.",
   "id": "de82f656a1705c29"
  },
  {
   "cell_type": "markdown",
   "id": "ad5603ad",
   "metadata": {},
   "source": [
    "Part 1 - Data Preprocessing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this section, we'll prepare our data for training the neural network.\n",
    "\n",
    "### Step 1: Import Required Libraries\n",
    "\n",
    "We need the following libraries:\n",
    "- numpy: For numerical operations\n",
    "- matplotlib: For creating visualizations\n",
    "- pandas: For data manipulation\n",
    "- seaborn: For enhanced visualizations\n"
   ],
   "id": "aa12deaa5acac6f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa02267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Load and Inspect the Dataset\n",
    "\n",
    "Load the Churn_Modelling.csv file and separate features:\n",
    "- X: Independent variables (columns 3-13)\n",
    "- y: Dependent variable (column 13, whether customer churned)\n",
    "\n",
    "We'll print the shapes to understand our data dimensions."
   ],
   "id": "782f0d13735ca5aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa6c71d03d1bff",
   "metadata": {},
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13]\n",
    "print(\"Independent features:\\n\", X)"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Shape of Independent features data:\\n\", X.shape)",
   "id": "fb023635"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y = dataset.iloc[:, 13]\n",
    "print(\"Dependent features:\\n\", y)"
   ],
   "id": "375db0b03087be12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Shape of Dependent features data:\\n\", y.shape)",
   "id": "718030048233c470"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Handle Categorical Variables\n",
    "\n",
    "Convert categorical variables (Geography and Gender) into numeric format using one-hot encoding.\n",
    "We use drop_first=True to avoid the dummy variable trap."
   ],
   "id": "b0c694be7bc0ed8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dummy variables\n",
    "geography=pd.get_dummies(X[\"Geography\"],drop_first=True)\n",
    "print(\"Shape of Geography data:\\n\", geography.shape)\n",
    "print(\"geography data:\\n\", geography)\n",
    "gender=pd.get_dummies(X['Gender'],drop_first=True)\n",
    "print(\"Shape of gender data:\\n\", gender.shape)\n",
    "print(\"gender data:\\n\", gender)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4: Combine Features\n",
    "\n",
    "Concatenate the one-hot encoded variables with our original features.\n",
    "This step creates our final feature matrix."
   ],
   "id": "439416fe879b2bad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate the Data Frames\n",
    "X=pd.concat([X,geography,gender],axis=1)\n",
    "print(\"Shape of X:\\n\", X.shape)\n",
    "print(\"Concatenated data:\\n\", X.values)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Clean Up Features\n",
    "\n",
    "Remove the original categorical columns since we now have their one-hot encoded versions.\n",
    "This prevents duplicate information in our dataset."
   ],
   "id": "3faed75e02209255"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6693edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Unnecessary columns\n",
    "X=X.drop(['Geography','Gender'],axis=1)\n",
    "print(\"Shape of X:\\n\", X.shape)\n",
    "print(\"Concatenated data after dropping unnecessary columns:\\n\", X.values)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 6: Split the Dataset\n",
    "\n",
    "Divide our data into training (80%) and testing (20%) sets.\n",
    "We use random_state=0 for reproducible results."
   ],
   "id": "f448c8264a1ec949"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(\"Shape of X_train:\\n\", X_train.shape)\n",
    "print(\"Shape of X_test:\\n\", X_test.shape)\n",
    "print(\"Shape of y_train:\\n\", y_train.shape)\n",
    "print(\"Shape of y_test:\\n\", y_test.shape)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 7: Feature Scaling\n",
    "\n",
    "Standardize our features using StandardScaler.\n",
    "This is crucial for neural networks to:\n",
    "- Ensure all features are on the same scale\n",
    "- Help with faster convergence\n",
    "- Prevent features with larger values from dominating the learning process"
   ],
   "id": "1dfe1199ca2b6977"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ce1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 2 - Building the Artificial Neural Network\n",
    "\n",
    "Now we'll create and train our neural network model."
   ],
   "id": "e3eebc85b77733a1"
  },
  {
   "cell_type": "markdown",
   "id": "d7dd2377",
   "metadata": {},
   "source": [
    "### Step 8: Import Neural Network Libraries\n",
    "\n",
    "Import Keras components needed for building our neural network:\n",
    "- Sequential: For creating the neural network\n",
    "- Dense: For adding fully connected layers\n",
    "- Dropout: For preventing overfittingPart 2 - Now let's make the ANN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d403ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 9: Initialize the Neural Network\n",
    "\n",
    "Create a Sequential model, which is a linear stack of layers."
   ],
   "id": "e48d1f265dd40f35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb56373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 10: Add First Hidden Layer\n",
    "\n",
    "Add the first hidden layer with:\n",
    "- 6 neurons (units)\n",
    "- ReLU activation function\n",
    "- He uniform initialization for weights\n",
    "- 11 input features (input_dim)"
   ],
   "id": "28cf2c26d95e38e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer\n",
    "# classifier.add(Dense(output_dim = 6, init = 'he_uniform',activation='relu',input_dim = 11))     ## older version\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu',input_dim = 11))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 11: Add Second Hidden Layer\n",
    "\n",
    "Add another hidden layer with:\n",
    "- 6 neurons\n",
    "- ReLU activation\n",
    "- He uniform initialization"
   ],
   "id": "bad79271f4f33ca7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu'))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 12: Add Output Layer\n",
    "\n",
    "Add the output layer with:\n",
    "- 1 neuron (binary classification)\n",
    "- Sigmoid activation (outputs probability between 0 and 1)\n",
    "- Glorot uniform initialization"
   ],
   "id": "a3a0bf839006d494"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 13: Compile the Model\n",
    "\n",
    "Configure the learning process with:\n",
    "- Adamax optimizer: An adaptive learning rate optimization algorithm\n",
    "- Binary cross-entropy loss: Suitable for binary classification\n",
    "- Accuracy metric: To monitor model performance"
   ],
   "id": "d0aa46ce0a0c10f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 14: Train the Model\n",
    "\n",
    "Train the neural network with:\n",
    "- 33% validation split\n",
    "- Batch size of 10\n",
    "- 100 epochs\n",
    "- Verbose output to monitor progress"
   ],
   "id": "6e2e2c4a79967d91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba184e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 10, epochs=100, verbose=1)      ## earlier version: nb_epoch=100\n",
    "print(\"model history:\\n\", model_history)\n",
    "# list all data in history"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 15: Visualize Training History\n",
    "\n",
    "Plot the training metrics:\n",
    "1. Accuracy plot: Shows how model accuracy improves over epochs\n",
    "2. Loss plot: Shows how the loss decreases over epochs\n",
    "\n",
    "Both plots compare training and validation metrics to detect overfitting."
   ],
   "id": "ff139cca7bf4f425"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e2e457948e809",
   "metadata": {},
   "source": [
    "# list all data in history\n",
    "print(model_history.history.keys())"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(model_history.history['accuracy'])             ## old version: plt.plot(model_history.history['acc'])\n",
    "plt.plot(model_history.history['val_accuracy'])         ## old version: plt.plot(model_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "id": "fabacf07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b53d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e85547",
   "metadata": {},
   "source": [
    "# Part 3 - Model Evaluation\n",
    "\n",
    "Now we'll evaluate our model's performance on the test set.\n",
    "\n",
    "### Step 16: Make Predictions\n",
    "\n",
    "Use the trained model to make predictions on test data:\n",
    "1. Get probability predictions\n",
    "2. Convert probabilities to binary predictions (threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22277a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 17: Create Confusion Matrix\n",
    "\n",
    "Generate and display the confusion matrix to show:\n",
    "- True Positives\n",
    "- True Negatives\n",
    "- False Positives\n",
    "- False Negatives"
   ],
   "id": "c4e02f9579ccb42a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 18: Calculate and Visualize Final Results\n",
    "\n",
    "1. Calculate the overall accuracy score\n",
    "2. Create a heatmap visualization of the confusion matrix\n",
    "3. Display final model performance metrics"
   ],
   "id": "f598f9cc6ae62eaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac36c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(y_pred,y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04446d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix (Accuracy = {score:.2f})')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
